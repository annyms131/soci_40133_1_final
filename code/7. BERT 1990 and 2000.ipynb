{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "J7ZjfEqkXhD1",
    "outputId": "a745553a-4b2a-4f71-ef62-2cf878c226b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
      "\r",
      "\u001b[K     |▋                               | 10kB 29.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 20kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 30kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 40kB 5.4MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 51kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 61kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 71kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 81kB 9.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 92kB 11.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 102kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 112kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 122kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 133kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 143kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 153kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 163kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 174kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 184kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 194kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 204kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 215kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 225kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 235kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 245kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 256kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 266kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 276kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 286kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 296kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 307kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 317kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 327kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 337kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 348kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 358kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 368kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 378kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 389kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 399kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 409kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 419kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 430kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 440kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 450kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 460kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 471kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 481kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 491kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 501kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 512kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 522kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 532kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 542kB 8.8MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\r",
      "\u001b[K     |▍                               | 10kB 34.4MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 20kB 42.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 30kB 48.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 40kB 53.7MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 51kB 56.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 61kB 59.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 71kB 61.3MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 81kB 61.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 92kB 63.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 102kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 112kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 122kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 133kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 143kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 153kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 163kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 174kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 184kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 194kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 204kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 215kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 225kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 235kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 245kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 256kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 266kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 276kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 286kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 296kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 307kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 317kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 327kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 337kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 348kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 358kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 368kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 378kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 389kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 399kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 409kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 419kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 430kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 440kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 450kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 460kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 471kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 481kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 491kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 501kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 512kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 522kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 532kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 542kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 552kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 563kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 573kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 583kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 593kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 604kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 614kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 624kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 634kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 645kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 655kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 665kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 675kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 686kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 696kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 706kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 716kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 727kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 737kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 747kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 757kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 768kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 778kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 788kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 798kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 808kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 819kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 829kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 839kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 849kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 860kB 65.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 870kB 65.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 28.0MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 30.8MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 30.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 33.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 36.2MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 40.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71kB 42.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 81kB 44.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 92kB 47.3MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 102kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 112kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 122kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 143kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 153kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 163kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 174kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 184kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 194kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 204kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 215kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 225kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 235kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 245kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 256kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 266kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 276kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 286kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 296kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 307kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 317kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 327kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 337kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 348kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 358kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 368kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 378kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 389kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 399kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 409kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 419kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 430kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 440kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 450kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 460kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 471kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 481kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 491kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 501kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 512kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 522kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 532kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 542kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 552kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 563kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 573kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 583kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 593kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 604kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 614kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 624kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 634kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 645kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 655kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 665kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 675kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 686kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 696kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 706kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 716kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 727kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 737kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 747kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 757kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 768kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 778kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 788kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 798kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 808kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 819kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 829kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 839kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 849kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 860kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 870kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 880kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 890kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 901kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 911kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 921kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 931kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 942kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 952kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 962kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 972kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 983kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 993kB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.0MB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.0MB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.0MB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.0MB 49.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.0MB 49.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 56.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=9b3025aaceebd8098d01c549d588bb9e8e5b36b4dfc47265b3a56185314d8b9c\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nm8tlyvnPV9K",
    "outputId": "d5a9a2bd-66d6-4a40-d17c-3317dd42333b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNHlxwSKMvO2"
   },
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) **and** also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different? \n",
    "\n",
    "Stretch: Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjVyxxH7Pqhc"
   },
   "source": [
    "*pre-processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "C6UhXDEnT9op",
    "outputId": "2e021f11-33f6-4302-d446-54c7b2595f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1T2tjRpzUaQV",
    "outputId": "6b39f3ef-6371-49b9-e8bc-3f47ad886d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajkZyhJ9UjQw"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Enk-1K-FUl0y",
    "outputId": "03aa33f4-382e-4b3b-d784-69904ec61066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=e95f97b1bd451d7fca8058f07e9b77a26f8e3b399f8036ab6f3dc23358087ba3\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Jq9fb34l04Fn",
    "outputId": "061d481b-d020-464e-c735-883a40d4f875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "Duh-dCnSUqA7",
    "outputId": "7a43ed02-c0a8-4c88-baa7-3fd5a2fda4e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-f82f10f8-c1ca-4448-9df6-fa2493f83166\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-f82f10f8-c1ca-4448-9df6-fa2493f83166\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df9000_train.tsv to df9000_train.tsv\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "# The URL for the dataset zip file.\n",
    "#url = 'https://github.com/yjnkwn/Content-Analysis-2020/blob/master/week_8/data/hw8_train.tsv'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "#if not os.path.exists('./hw8_train.tsv'):\n",
    "#    wget.download(url, './hw8_train.tsv')\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-zVruU6NfrR"
   },
   "source": [
    "Put your data into the format BERT expects.\n",
    "\n",
    "\n",
    "*   Column 1: An ID for the row (can be just a count, or even just the same number or letter for every row, if you don’t care to keep track of each individual example).\n",
    "\n",
    "*   Column 2: A label for the row as an int. These are the classification labels that your classifier aims to predict.\n",
    "\n",
    "*   Column 3: A column of all the same letter — this is a throw-away column that you need to include because the BERT model expects it.\n",
    "\n",
    "*   Column 4: The text examples you want to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "id": "WSpm5fy_XZfr",
    "outputId": "faef754e-c39c-48bb-a9e0-3687d1c7274d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 1,200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1574737</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>It is argued that the prevalence of AIDS is su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>18846895</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Children and youth with cerebral palsy (CP) ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>8262170</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Data from an urban obstetrics clinic on 291 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1918226</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Both high expressed emotion (EE) and psychiatr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>8151506</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Although peer raters of personality traits do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>11483930</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>OBJECTIVES: This article explores whether the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>12179757</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Korea completed the whole process of what is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>9850877</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>OBJECTIVES: The social conditions under which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>18582145</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Depression in older adults has been detected, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>24441483</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The study investigated relations between the q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  ...                                           sentence\n",
       "751          1574737  ...  It is argued that the prevalence of AIDS is su...\n",
       "955         18846895  ...  Children and youth with cerebral palsy (CP) ex...\n",
       "780          8262170  ...  Data from an urban obstetrics clinic on 291 un...\n",
       "410          1918226  ...  Both high expressed emotion (EE) and psychiatr...\n",
       "144          8151506  ...  Although peer raters of personality traits do ...\n",
       "454         11483930  ...  OBJECTIVES: This article explores whether the ...\n",
       "490         12179757  ...  Korea completed the whole process of what is c...\n",
       "755          9850877  ...  OBJECTIVES: The social conditions under which ...\n",
       "571         18582145  ...  Depression in older adults has been detected, ...\n",
       "486         24441483  ...  The study investigated relations between the q...\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(io.BytesIO(uploaded['df9000_train.tsv']), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "Kf90pLBb_SMe",
    "outputId": "4fda932f-5f1a-4b16-f735-4942f373fd20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>There is an ongoing debate about the relative ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>The American Cancer Society (ACS) Study of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>BACKGROUND: The inappropriate use of emergency...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>BACKGROUND: Characterized native and recombina...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The object of the paper is to assess the socia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "179   There is an ongoing debate about the relative ...      0\n",
       "638   The American Cancer Society (ACS) Study of the...      0\n",
       "55    BACKGROUND: The inappropriate use of emergency...      0\n",
       "1187  BACKGROUND: Characterized native and recombina...      0\n",
       "110   The object of the paper is to assess the socia...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mk7LS-8hUxwS"
   },
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142,
     "referenced_widgets": [
      "e826c9ea60404d36a8529fbf5a5b5254",
      "ee20e52d9321434d9a58a069d0580293",
      "02363b9b04244b259e913c0dfcdb5f88",
      "0b28ea559334497ea28e202b6ba6b13c",
      "38c7ca8437604730b69e1a7da4331493",
      "a4014ba66f1b42c2bf4fdd42bdf3920a",
      "ee1eeb7818294363894fffd12f942323",
      "715d21252e0f47fda801815d69e8f4e7"
     ]
    },
    "colab_type": "code",
    "id": "3v0hizTPUx0b",
    "outputId": "abb0cab1-8f47-4da7-81f2-2d7cf23de4bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e826c9ea60404d36a8529fbf5a5b5254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first sentence:\n",
      "['[CLS]', '\"', 'this', 'essay', 'presents', 'tentative', 'findings', 'for', 'the', 'sun', '##ds', '##val', '##l', 'area', 'of', 'sweden', ',', 'which', 'is', 'being', 'studied', 'in', 'a', 'recent', 'project', 'on', 'the', 'decline', 'of', 'infant', 'and', 'childhood', 'mortality', 'in', 'the', 'nordic', 'countries', '.', 'the', 'focus', 'is', 'on', 'the', 'complete', 'reproductive', 'histories', 'of', 'single', 'mothers', 'and', 'the', 'life', 'expect', '##an', '##cies', 'among', 'infants', 'born', 'to', 'women', 'who', 'at', 'least', 'once', 'in', 'their', 'reproductive', 'life', 'history', 'experienced', 'the', 'birth', 'of', 'an', 'illegitimate', 'child', '.', 'in', 'sun', '##ds', '##val', '##l', ',', 'industrial', '##ization', 'only', 'temporarily', 'affected', 'the', 'ill', '##eg', '##iti', '##mac', '##y', 'ratio', ',', 'but', 'its', 'effect', 'was', 'obvious', 'even', 'in', 'agrarian', 'parishes', '.', 'the', 'number', 'of', 'illegitimate', 'children', 'per', 'woman', 'remained', 'relatively', 'stable', 'over', 'time', ',', 'with', 'the', 'exception', 'of', 'the', 'town', 'of', 'sun', '##ds', '##val', '##l', '.', 'it', 'was', 'more', 'common', 'in', 'the', 'urban', 'environment', 'to', 'give', 'birth', 'to', 'several', 'illegitimate', 'children', '.', 'mortality', 'was', 'also', 'higher', 'among', 'these', 'infants', ',', 'but', 'the', 'negative', 'effects', 'can', 'be', 'seen', 'equally', 'among', 'both', 'legitimate', 'and', 'illegitimate', 'children', '.', '\"', '[SEP]']\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n",
    "print(len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALVfOtybUx4h"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_rMpuwpUx87"
   },
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vy2VK-86Ux_v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOcE-n5wUyDY"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8t33dXlUyGk"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qW4JdDVdVOzt"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyiyDiD-TiOi"
   },
   "source": [
    "**Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsXWUCyJVO95"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhiRbh56VPBH"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxEaVbz5Vxi7"
   },
   "source": [
    "**Loading our Models**\n",
    "\n",
    "**Train Model**\n",
    "\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model. For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "\n",
    "**Structure of Fine-Tuning Model**\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "**The Fine-Tuning Process**\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task. Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sgDgkDJiryaW",
    "outputId": "1aeeaaf1-824c-4737-e7c1-f2a14f511e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c62571d4349d4151be1e3c4e67690dfb",
      "262eb2486b3e446287c55770a0e092a9",
      "89126c0bb81a4256881c4f01b79195b4",
      "eca3191c9214461a98507870e6da9226",
      "9e84e41db44c45a18aed5c89bf6b0c5a",
      "568e21ea1a2c45209e85d7c20c8c84f0",
      "05bed9805ca84472886c9c99037d7643",
      "155398f72daf4e2a8b98e3353bc44db3",
      "eb276821b5bd47cea696ff91cf84e4c4",
      "42cc84b568b743cb88d576a4519d93cf",
      "86782811953c42c0bdb564b52ca4bd7b",
      "d6adf78618984aa990ca4a25faca9a4d",
      "c9efff0c3d104e129045b2739b930aad",
      "51e8354a8148461094938805184e29a3",
      "a7382f795e5e4f9c9640763412b422cf",
      "9b3b329dc9fb4f719b263f50f7b2b2d0"
     ]
    },
    "colab_type": "code",
    "id": "_lAU3erTVPJc",
    "outputId": "5e3ba53b-0b7e-416b-c8ba-da65c6fa7630"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62571d4349d4151be1e3c4e67690dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb276821b5bd47cea696ff91cf84e4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels= 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vo7LpBSNVPMl"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVuTQB4jVPQq"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQ4OjCDoVPT_"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrLCvSsXVPYX"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFerE07QVPbv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "colab_type": "code",
    "id": "EyO_Da_qVPfF",
    "outputId": "134472cc-9d7f-45ef-d37b-ad594876ce6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epcoh took: 0:00:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.62\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epcoh took: 0:00:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epcoh took: 0:00:25\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.42\n",
      "  Training epcoh took: 0:00:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.72\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        accuracy = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnKSZmq3YLuM"
   },
   "source": [
    "**Training Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Gj1XHF4OWlj6",
    "outputId": "1e842f8d-5046-4769-cd9d-1659d7d9af03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6673421631841099,\n",
       " 0.5883907114758211,\n",
       " 0.4940085831810446,\n",
       " 0.41979671225828286]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do9xKdBGg2z6"
   },
   "outputs": [],
   "source": [
    "# accuracy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8PSGm0YhWCs"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# create a new function defining a style\n",
    "def set_style():\n",
    "    \n",
    "    # Set reasonable defaults for font size for figure that will go in a paper\n",
    "    sns.set_context(\"paper\")\n",
    "    \n",
    "    # Set the font to be serif, rather than sans\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    # Make the background white, and specify the\n",
    "    # specific font family\n",
    "    sns.set_style(\"white\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "    })\n",
    "    \n",
    "# call the function    \n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "id": "LzIN_TMZJPSH",
    "outputId": "ba656ab5-b92b-44d4-a7c7-c50f5069f42e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGuCAYAAAD2yjJgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIdMj/Ay6vRzYDBm+7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title('Training Loss\\n Bert (batch = 32)')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tS1BK-s2J3zR"
   },
   "source": [
    "**Holdout Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "2GfCmETDAb69",
    "outputId": "ca22572a-d69f-466f-93d6-8ca21f8d4235"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-183fc266-51f8-40c4-9243-fa78677d3fb2\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-183fc266-51f8-40c4-9243-fa78677d3fb2\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df9000_dev.tsv to df9000_dev.tsv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "KtuQBIDAWlnE",
    "outputId": "3354d4f8-8437-4691-ba58-fae2fda51b5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(io.BytesIO(uploaded['df9000_dev.tsv']), delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QSMUrXteWlqd",
    "outputId": "bc89126b-9740-4ac8-bd72-4c5fed410e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 400 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CZM-s5PFWlvP",
    "outputId": "68ddb1d2-bf86-4e00-bdd2-20780a8c1b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 205 of 400 (51.25%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "m5rik1UwWlyO",
    "outputId": "3130a9f4-e234-49a0-a3a0-d3988feced9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "_-cpfuraWl3D",
    "outputId": "4bc702e1-dfa9-42c9-d496-5576c54774be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4365641250653994,\n",
       " 0.629940788348712,\n",
       " 0.43393264597022163,\n",
       " 0.44971201491459334,\n",
       " 0.6180700462007377,\n",
       " 0.3730235484764954,\n",
       " 0.3779644730092272,\n",
       " 0.5219786367558533,\n",
       " 0.31311214554257477,\n",
       " 0.5636018619766345,\n",
       " 0.4248710287154289,\n",
       " 0.4365641250653994,\n",
       " 0.674199862463242]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QzXH9hlSWl6L",
    "outputId": "bc47be1a-e179-45b4-9f68-0453b2ccb98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.465\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBnqPd4bKkh3"
   },
   "source": [
    "Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UnKSZmq3YLuM",
    "vpZLQ2jBZNrM"
   ],
   "name": "Neural Network Classification (90's vs 00's)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02363b9b04244b259e913c0dfcdb5f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4014ba66f1b42c2bf4fdd42bdf3920a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38c7ca8437604730b69e1a7da4331493",
      "value": 231508
     }
    },
    "05bed9805ca84472886c9c99037d7643": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b28ea559334497ea28e202b6ba6b13c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_715d21252e0f47fda801815d69e8f4e7",
      "placeholder": "​",
      "style": "IPY_MODEL_ee1eeb7818294363894fffd12f942323",
      "value": " 232k/232k [00:00&lt;00:00, 812kB/s]"
     }
    },
    "155398f72daf4e2a8b98e3353bc44db3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "262eb2486b3e446287c55770a0e092a9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38c7ca8437604730b69e1a7da4331493": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "42cc84b568b743cb88d576a4519d93cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51e8354a8148461094938805184e29a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "568e21ea1a2c45209e85d7c20c8c84f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "715d21252e0f47fda801815d69e8f4e7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86782811953c42c0bdb564b52ca4bd7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51e8354a8148461094938805184e29a3",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9efff0c3d104e129045b2739b930aad",
      "value": 440473133
     }
    },
    "89126c0bb81a4256881c4f01b79195b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_568e21ea1a2c45209e85d7c20c8c84f0",
      "max": 361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e84e41db44c45a18aed5c89bf6b0c5a",
      "value": 361
     }
    },
    "9b3b329dc9fb4f719b263f50f7b2b2d0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e84e41db44c45a18aed5c89bf6b0c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a4014ba66f1b42c2bf4fdd42bdf3920a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7382f795e5e4f9c9640763412b422cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c62571d4349d4151be1e3c4e67690dfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89126c0bb81a4256881c4f01b79195b4",
       "IPY_MODEL_eca3191c9214461a98507870e6da9226"
      ],
      "layout": "IPY_MODEL_262eb2486b3e446287c55770a0e092a9"
     }
    },
    "c9efff0c3d104e129045b2739b930aad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d6adf78618984aa990ca4a25faca9a4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b3b329dc9fb4f719b263f50f7b2b2d0",
      "placeholder": "​",
      "style": "IPY_MODEL_a7382f795e5e4f9c9640763412b422cf",
      "value": " 440M/440M [00:15&lt;00:00, 27.8MB/s]"
     }
    },
    "e826c9ea60404d36a8529fbf5a5b5254": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02363b9b04244b259e913c0dfcdb5f88",
       "IPY_MODEL_0b28ea559334497ea28e202b6ba6b13c"
      ],
      "layout": "IPY_MODEL_ee20e52d9321434d9a58a069d0580293"
     }
    },
    "eb276821b5bd47cea696ff91cf84e4c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_86782811953c42c0bdb564b52ca4bd7b",
       "IPY_MODEL_d6adf78618984aa990ca4a25faca9a4d"
      ],
      "layout": "IPY_MODEL_42cc84b568b743cb88d576a4519d93cf"
     }
    },
    "eca3191c9214461a98507870e6da9226": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_155398f72daf4e2a8b98e3353bc44db3",
      "placeholder": "​",
      "style": "IPY_MODEL_05bed9805ca84472886c9c99037d7643",
      "value": " 361/361 [00:00&lt;00:00, 911B/s]"
     }
    },
    "ee1eeb7818294363894fffd12f942323": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee20e52d9321434d9a58a069d0580293": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
